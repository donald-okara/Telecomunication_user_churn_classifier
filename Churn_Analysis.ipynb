{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecommunication_user_churn_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Mining\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "#print(\"Libraries imported successfully.....\")\n",
    "#Importing data\n",
    "df = pd.read_csv(\"telecom_customer_churn.csv\")\n",
    "\n",
    "#print(\"Data imported successfully.....\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns \"Customer Status\", \"Churn Category\", and \"Churn Reason\"\n",
    "columns_to_remove = [\"Customer Status\", \"Churn Category\", \"Churn Reason\"]\n",
    "df_train = df.drop(columns=columns_to_remove)\n",
    "\n",
    "# Take out 30% of the DataFrame as the test set\n",
    "test_size = 0.3\n",
    "df_test = df_train.sample(frac=test_size, random_state=42)\n",
    "df = df.drop(index=df_test.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Cleaning and EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate numerical and categorical columns\n",
    "\n",
    "from preprolib import myfunctions\n",
    "num_cols = []\n",
    "cat_cols = []\n",
    "\n",
    "ignore_list = ['Zip Code', 'Longitude', 'Latitude', \n",
    "                'Customer ID', 'Churn Category', \n",
    "                'Churn Reason', 'Customer Status', 'City']\n",
    "\n",
    "myfunctions.cat_or_num(df, ignore_list, num_cols, cat_cols)\n",
    "\n",
    "label = 'Customer Status'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1 Numerical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##num_cols.remove\n",
    "\n",
    "remove_num = ['Age', 'Avg Monthly Long Distance Charges',  'Avg Monthly GB Download', 'Monthly Charge']\n",
    "\n",
    "num_cols = [x for x in num_cols if x not in remove_num]\n",
    "\n",
    "\n",
    "##cat_cols.remove\n",
    "\n",
    "remove_cat = ['Gender', 'Married',  'Multiple Lines']\n",
    "\n",
    "cat_cols = [x for x in cat_cols if x not in remove_cat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(df[col], bins=20, edgecolor='k', alpha=0.7)\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical data\n",
    "for column in num_cols:\n",
    "    plt.figure()  # Create a new figure for each box plot\n",
    "    sns.boxplot(x='Customer Status', y=column, data=df)\n",
    "    plt.title(f'Box Plot for {column} against {\"Customer Status\"}')\n",
    "    plt.ylabel(column)\n",
    "    plt.xlabel('Target')\n",
    "    plt.show()\n",
    "\n",
    "num_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Categorical data\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disstribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cat_counts = df[col].value_counts()\n",
    "    cat_counts.plot(kind='bar', edgecolor='k', alpha=0.7)\n",
    "    plt.title(f'Bar Plot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Define interactive widgets\n",
    "feature_dropdown = widgets.Dropdown(\n",
    "    options=cat_cols,\n",
    "    description='Feature:'\n",
    ")\n",
    "\n",
    "status1_dropdown = widgets.Dropdown(\n",
    "    options=df['Customer Status'].unique(),\n",
    "    description='Status 1:'\n",
    ")\n",
    "\n",
    "status2_dropdown = widgets.Dropdown(\n",
    "    options=df['Customer Status'].unique(),\n",
    "    description='Status 2:'\n",
    ")\n",
    "\n",
    "# Create an output widget for displaying the visualization\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to create and display pie charts\n",
    "def create_and_display_pie_charts(feature, status1, status2):\n",
    "    with output:\n",
    "        clear_output(wait=True)  # Clear the previous output\n",
    "        filtered_df = df[(df['Customer Status'].isin([status1, status2]))]\n",
    "\n",
    "        # Create a subplot for each category within the selected feature\n",
    "        unique_categories = filtered_df[feature].unique()\n",
    "        num_categories = len(unique_categories)\n",
    "\n",
    "        fig, axes = plt.subplots(1, num_categories, figsize=(15, 5))\n",
    "\n",
    "        for i, category in enumerate(unique_categories):\n",
    "            category_counts = filtered_df[filtered_df[feature] == category]['Customer Status'].value_counts()\n",
    "            ax = axes[i]\n",
    "            ax.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "            ax.set_title(category)\n",
    "            ax.axis('equal')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Define the interactive function\n",
    "def update_pie_charts(change):\n",
    "    feature = feature_dropdown.value\n",
    "    status1 = status1_dropdown.value\n",
    "    status2 = status2_dropdown.value\n",
    "\n",
    "    create_and_display_pie_charts(feature, status1, status2)\n",
    "\n",
    "# Set up the interactivity\n",
    "feature_dropdown.observe(update_pie_charts, names='value')\n",
    "status1_dropdown.observe(update_pie_charts, names='value')\n",
    "status2_dropdown.observe(update_pie_charts, names='value')\n",
    "\n",
    "# Display the widgets and output area\n",
    "display(widgets.VBox([feature_dropdown, status1_dropdown, status2_dropdown]))\n",
    "display(output)\n",
    "\n",
    "# Display initial pie charts based on default selections\n",
    "create_and_display_pie_charts(cat_cols[0], df['Customer Status'].unique()[0], df['Customer Status'].unique()[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and label\n",
    "features = cat_cols + num_cols\n",
    "label = 'Customer Status'\n",
    "\n",
    "# Convert the label column to ordinal categories\n",
    "label_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "y = label_encoder.fit_transform(df[label].values.reshape(-1, 1))\n",
    "\n",
    "# Function to encode based on the percentage of churn per category\n",
    "def custom_encode(column, churn_column):\n",
    "    churn_percentage = df.groupby(column)[churn_column].mean()\n",
    "    return churn_percentage.rank().fillna(0).astype(int)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Define a pipeline for numeric columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define a pipeline for categorical columns\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# Create a ColumnTransformer to apply the pipeline to the numeric and categorical columns\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', cat_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit the preprocessor to the training data and transform both the training and test data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print('Training Set: %d, Test Set: %d \\n' % (len(X_train), len(X_test)))\n",
    "\n",
    "# Print the transformed DataFrames\n",
    "print(\"X_train_Transformed:\\n\", X_train_transformed)\n",
    "print(\"\\nX_test_Transformed:\\n\", X_test_transformed)\n",
    "\n",
    "\n",
    "# Create a dictionary to map encoded variables to original labels\n",
    "label_mapping = {}\n",
    "for i, label in enumerate(label_encoder.categories_[0]):\n",
    "    label_mapping[i] = label\n",
    "\n",
    "# Print the dictionary\n",
    "print(\"Encoded to Original Label Mapping:\")\n",
    "print(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Model Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=1).fit(X_train_transformed, y_train)  # Convert one-hot encoded y_train to 1D array\n",
    "\n",
    "\n",
    "#Model evaluation\n",
    "#from preprolib.myfunctions import evaluate_model\n",
    "\n",
    "#evaluate_model(rf_model,X_test_transformed, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(curr_model, X_test, y_test):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    #Convert the one-hot encoded y_test to a 1-dimensional array\n",
    "    y_test = y_test\n",
    "\n",
    "    predictions = curr_model.predict(X_test)\n",
    "\n",
    "\n",
    "    #Calculate the necessary metrics\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "    #Print the results for the current model\n",
    "    print(\"Model Evaluation:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "evaluate_model(rf_model,X_test_transformed, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation with 5 folds (you can adjust the number of folds as needed)\n",
    "\n",
    "cv_scores = cross_val_score(rf_model, X_train_transformed, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation Scores:\", cv_scores)\n",
    "print(\"Average Cross-validation Accuracy:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(rf_model, 'trained_rf_model.pkl')\n",
    "\n",
    "print(\"Model saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "loaded_rf_model = joblib.load('trained_rf_model.pkl')\n",
    "\n",
    "# Transform the test data using the preprocessor\n",
    "df_test_transformed = preprocessor.transform(df_test)\n",
    "\n",
    "# Make predictions on the test data using the loaded model\n",
    "test_pred = loaded_rf_model.predict(df_test_transformed)\n",
    "\n",
    "# Reshape the test_pred array to a 2D array\n",
    "test_pred_2d = test_pred.reshape(-1, 1)\n",
    "\n",
    "# Map the predicted values back to their original labels\n",
    "predicted_labels = label_encoder.inverse_transform(test_pred_2d)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\")\n",
    "print(predicted_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the predicted labels\n",
    "predicted_df = pd.DataFrame(predicted_labels, columns=['Predicted_Customer_Status'])\n",
    "\n",
    "# Concatenate the predicted_df with df_test\n",
    "df_test_with_predictions = pd.concat([df_test, predicted_df], axis=1)\n",
    "\n",
    "# Print the DataFrame with the predicted labels\n",
    "print(df_test_with_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
