{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "print(\"Libraries imported successfully.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data\n",
    "df = pd.read_csv(\"telecom_customer_churn.csv\")\n",
    "\n",
    "print(\"Data imported successfully.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate numerical and categorical columns\n",
    "\n",
    "from preprolib import myfunctions\n",
    "num_cols = []\n",
    "cat_cols = []\n",
    "\n",
    "ignore_list = ['Zip Code', 'Longitude', 'Latitude', \n",
    "                'Customer ID', 'Churn Category', \n",
    "                'Churn Reason', 'Customer Status', 'City']\n",
    "\n",
    "myfunctions.cat_or_num(df, ignore_list, num_cols, cat_cols)\n",
    "\n",
    "label = 'Customer Status'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6 * len(cat_cols)))\n",
    "\n",
    "for idx, cat_col in enumerate(cat_cols, start=1):\n",
    "    plt.subplot(len(cat_cols), 1, idx)\n",
    "    sns.countplot(data=df, x=cat_col, hue='Customer Status', palette='pastel')\n",
    "    plt.title(f\"Count of 'Stayed', 'Churn' or 'Joined' for {cat_col}\")\n",
    "    plt.xlabel(cat_col)\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "from scipy.stats import chi2_contingency\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributions for all numeric variables \n",
    "for i in df[num_cols]:\n",
    "    plt.hist(df[num_cols][i])\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[num_cols].corr())\n",
    "sns.heatmap(df[num_cols].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and label\n",
    "features = cat_cols + num_cols\n",
    "label = 'Customer Status'\n",
    "\n",
    "# Convert the label column to ordinal categories\n",
    "label_encoder = OrdinalEncoder()\n",
    "y = label_encoder.fit_transform(df[label].values.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Define a pipeline for numeric columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define a pipeline for categorical columns\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create a ColumnTransformer to apply the pipeline to the numeric and categorical columns\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', cat_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit the preprocessor to the training data and transform both the training and test data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print('Training Set: %d, Test Set: %d \\n' % (len(X_train), len(X_test)))\n",
    "\n",
    "# Print the transformed DataFrames\n",
    "print(\"X_train_Transformed:\\n\", X_train_transformed)\n",
    "print(\"\\nX_test_Transformed:\\n\", X_test_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier#\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#mcm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "lr_model = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=10000).fit(X_train_transformed, y_train)\n",
    "kn_model = KNeighborsClassifier().fit(X_train_transformed, y_train)\n",
    "dt_model = DecisionTreeClassifier().fit(X_train_transformed, y_train)\n",
    "rf_model = RandomForestClassifier().fit(X_train_transformed, y_train)\n",
    "nb_model = GaussianNB().fit(X_train_transformed, y_train)\n",
    "ab_model = AdaBoostClassifier().fit(X_train_transformed, y_train)\n",
    "#svc_model = SVC().fit(X_train_transformed,y_train)\n",
    "\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', lr_model),\n",
    "    ('K-Nearest Neighbors', kn_model),\n",
    "    ('Decision Tree', dt_model),\n",
    "    ('Random Forest', rf_model),\n",
    "    ('Gaussian Naive Bayes', nb_model),\n",
    "    ('AdaBoost', ab_model)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                 Model    Recall  F1-score  Precision  Accuracy\n",
      "0  Logistic Regression  0.825367  0.825931   0.827337  0.825367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\NewEnv\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                 Model    Recall  F1-score  Precision  Accuracy\n",
      "0  K-Nearest Neighbors  0.750592  0.749405   0.754928  0.750592\n",
      "\n",
      "Classification Report:\n",
      "           Model    Recall  F1-score  Precision  Accuracy\n",
      "0  Decision Tree  0.773781  0.777225   0.782173  0.773781\n",
      "\n",
      "Classification Report:\n",
      "           Model    Recall  F1-score  Precision  Accuracy\n",
      "0  Random Forest  0.836252   0.83073   0.829757  0.836252\n",
      "\n",
      "Classification Report:\n",
      "                  Model    Recall  F1-score  Precision  Accuracy\n",
      "0  Gaussian Naive Bayes  0.730241  0.745078   0.790164  0.730241\n",
      "\n",
      "Classification Report:\n",
      "      Model    Recall  F1-score  Precision  Accuracy\n",
      "0  AdaBoost  0.751065  0.693595   0.668214  0.751065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\NewEnv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define lists to store model evaluation results\n",
    "\n",
    "\n",
    "def evaluate_model(curr_model, model_name, X_test, y_test):\n",
    "    model_names = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    accuracy_scores = []\n",
    "    print(f'\\nCurrent model is: {model_name}')\n",
    "    predictions = curr_model.predict(X_test)\n",
    "\n",
    "    # Calculate the necessary metrics\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "    # Append model name and metric scores to the respective lists\n",
    "    model_names.append(model_name)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Print classification report\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "# Example usage\n",
    "for model_name, model in models:\n",
    "    evaluate_model(model, model_name, X_test_transformed, y_test)\n",
    "\n",
    "# Create a DataFrame to store the model evaluation results\n",
    "model_evaluation_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Recall': recall_scores,\n",
    "    'F1-score': f1_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Accuracy': accuracy_scores\n",
    "})\n",
    "model_evaluation_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_evaluation_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\telecom_customer_churn.csv\\Analysis.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/telecom_customer_churn.csv/Analysis.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Print the DataFrame with model evaluation results\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/telecom_customer_churn.csv/Analysis.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Sort the DataFrame by accuracy in descending order\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/telecom_customer_churn.csv/Analysis.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_evaluation_df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/telecom_customer_churn.csv/Analysis.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_evaluation_df\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/telecom_customer_churn.csv/Analysis.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mModel Evaluation:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_evaluation_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the DataFrame with model evaluation results\n",
    "# Sort the DataFrame by accuracy in descending order\n",
    "model_evaluation_df.sort_values(by='Accuracy', ascending=False, inplace=True)\n",
    "model_evaluation_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(model_evaluation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
